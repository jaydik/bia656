In [10]: import pandas as pdimport numpy as npimport matplotlib.pyplot as pltIn [ ]: ?In [ ]: ?In [ ]: ? ## Import DataIn [21]: train = pd.read_csv('../cleaneddata/cleanedTraining.csv', index_col=0)In [22]: train.dtypesOut[22]:0                       float64 1                       float64 2                       float64 3                       float64 4                       float64 5                       float64 6                       float64 7                       float64 8                       float64 9                       float64 10                      float64 11                      float64 12                      float64 13                      float64 14                      float64 LogSales                float64 DayOfWeek                 int64 Store                     int64 WeekNumber                int64 Observed                float64 Trend                   float64 MeanCustomerObserved    float64 dtype: objectIn [16]: #train.isnull().sum()In [ ]: ?In [ ]: ?In [ ]: ? ?Train/Test SplitIn [41]: from sklearn.cross_validation import train_test_split?X_train, X_test, y_train, y_test = train_test_split(train.drop(['LogSales'], axis=1),                                                     train['LogSales'],                                                     test_size=0.25,                                                     random_state=123)In [42]: from sklearn.tree import DecisionTreeRegressor?explore_tree = DecisionTreeRegressor().fit(X_train, y_train)pred = explore_tree.predict(X_test)In [43]: from sklearn.metrics import mean_squared_errorfrom math import sqrt?rmse = sqrt(mean_squared_error(pred, y_test))?print(rmse)0.179174826577 In [44]: %matplotlib inlinefeature_importance = rf.feature_importances_# make importances relative to max importancefeature_importance = 100.0 * (feature_importance / feature_importance.max())sorted_idx = np.argsort(feature_importance)pos = np.arange(sorted_idx.shape[0]) + .5plt.subplot(1, 2, 2)plt.barh(pos, feature_importance[sorted_idx], align='center')plt.yticks(pos, X_train.columns[sorted_idx])plt.xlabel('Relative Importance')plt.title('Variable Importance')plt.show()In [53]: import xgboost as xgb?def ToWeight(y):    w = np.zeros(y.shape, dtype=float)    ind = y != 0    w[ind] = 1./(y[ind]**2)    return w??def rmspe(yhat, y):    w = ToWeight(y)    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))    return rmspe??def rmspe_xg(yhat, y):    # y = y.values    y = y.get_label()    y = np.exp(y) - 1    yhat = np.exp(yhat) - 1    w = ToWeight(y)    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))    return "rmspe", rmspeIn [54]: val_size = 15000??params = {"objective": "reg:linear",          "eta": 0.2,          "max_depth": 12,          "subsample": 0.7,          "colsample_bytree": 0.7,          "silent": 1          }num_trees = 2000dtrain = xgb.DMatrix(X_train, y_train)dvalid = xgb.DMatrix(X_test, y_test)#dtest = xgb.DMatrix(test)watchlist = [(dtrain, 'train'),(dvalid, 'eval')]gbm5 = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, feval=rmspe_xg, verbose_eval=True)?Will train until eval error hasn't decreased in 50 rounds. [0]	train-rmspe:0.998726	eval-rmspe:0.998725 [1]	train-rmspe:0.994787	eval-rmspe:0.994782 [2]	train-rmspe:0.984909	eval-rmspe:0.984896 [3]	train-rmspe:0.964969	eval-rmspe:0.964928 [4]	train-rmspe:0.931526	eval-rmspe:0.931410 [5]	train-rmspe:0.883090	eval-rmspe:0.882799 [6]	train-rmspe:0.820820	eval-rmspe:0.820125 [7]	train-rmspe:0.748318	eval-rmspe:0.746857 [8]	train-rmspe:0.670257	eval-rmspe:0.667465 [9]	train-rmspe:0.591744	eval-rmspe:0.586912 [10]	train-rmspe:0.516845	eval-rmspe:0.509281 [11]	train-rmspe:0.449321	eval-rmspe:0.437769 [12]	train-rmspe:0.390719	eval-rmspe:0.374337 [13]	train-rmspe:0.341856	eval-rmspe:0.320164 [14]	train-rmspe:0.303999	eval-rmspe:0.275561 [15]	train-rmspe:0.271742	eval-rmspe:0.239989 [16]	train-rmspe:0.251194	eval-rmspe:0.213086 [17]	train-rmspe:0.237230	eval-rmspe:0.193139 [18]	train-rmspe:0.227519	eval-rmspe:0.178833 [19]	train-rmspe:0.221911	eval-rmspe:0.169174 [20]	train-rmspe:0.213415	eval-rmspe:0.163076 [21]	train-rmspe:0.211688	eval-rmspe:0.159443 [22]	train-rmspe:0.210905	eval-rmspe:0.157562 [23]	train-rmspe:0.210441	eval-rmspe:0.156500 [24]	train-rmspe:0.209062	eval-rmspe:0.156102 [25]	train-rmspe:0.208752	eval-rmspe:0.155434 [26]	train-rmspe:0.208910	eval-rmspe:0.155313 [27]	train-rmspe:0.209026	eval-rmspe:0.155293 [28]	train-rmspe:0.206198	eval-rmspe:0.155256 [29]	train-rmspe:0.206080	eval-rmspe:0.155238 [30]	train-rmspe:0.206435	eval-rmspe:0.155446 [31]	train-rmspe:0.205644	eval-rmspe:0.154641 [32]	train-rmspe:0.205685	eval-rmspe:0.154496 [33]	train-rmspe:0.204051	eval-rmspe:0.154421 [34]	train-rmspe:0.202285	eval-rmspe:0.153750 [35]	train-rmspe:0.202041	eval-rmspe:0.153634 [36]	train-rmspe:0.201905	eval-rmspe:0.153482 [37]	train-rmspe:0.200913	eval-rmspe:0.153237 [38]	train-rmspe:0.200530	eval-rmspe:0.153178 [39]	train-rmspe:0.198973	eval-rmspe:0.152954 [40]	train-rmspe:0.198765	eval-rmspe:0.152855 [41]	train-rmspe:0.197731	eval-rmspe:0.152098 [42]	train-rmspe:0.197384	eval-rmspe:0.151721 [43]	train-rmspe:0.197200	eval-rmspe:0.151615 [44]	train-rmspe:0.196951	eval-rmspe:0.151517 [45]	train-rmspe:0.196440	eval-rmspe:0.151061 [46]	train-rmspe:0.196046	eval-rmspe:0.150913 [47]	train-rmspe:0.195560	eval-rmspe:0.150695 [48]	train-rmspe:0.195317	eval-rmspe:0.150521 [49]	train-rmspe:0.195104	eval-rmspe:0.150511 [50]	train-rmspe:0.195012	eval-rmspe:0.150352 [51]	train-rmspe:0.194720	eval-rmspe:0.150119 [52]	train-rmspe:0.193881	eval-rmspe:0.149344 [53]	train-rmspe:0.193731	eval-rmspe:0.149221 [54]	train-rmspe:0.193336	eval-rmspe:0.149144 [55]	train-rmspe:0.192854	eval-rmspe:0.148949 [56]	train-rmspe:0.191846	eval-rmspe:0.148647 [57]	train-rmspe:0.191643	eval-rmspe:0.148511 [58]	train-rmspe:0.191520	eval-rmspe:0.148395 [59]	train-rmspe:0.191263	eval-rmspe:0.148264 [60]	train-rmspe:0.191255	eval-rmspe:0.148265 [61]	train-rmspe:0.172715	eval-rmspe:0.148139 [62]	train-rmspe:0.172473	eval-rmspe:0.148117 [63]	train-rmspe:0.171587	eval-rmspe:0.147482 [64]	train-rmspe:0.171177	eval-rmspe:0.147412 [65]	train-rmspe:0.170978	eval-rmspe:0.147176 [66]	train-rmspe:0.170630	eval-rmspe:0.147173 [67]	train-rmspe:0.170317	eval-rmspe:0.146832 [68]	train-rmspe:0.170219	eval-rmspe:0.146724 [69]	train-rmspe:0.170193	eval-rmspe:0.146705 [70]	train-rmspe:0.169762	eval-rmspe:0.146660 [71]	train-rmspe:0.169237	eval-rmspe:0.146298 [72]	train-rmspe:0.169460	eval-rmspe:0.145864 [73]	train-rmspe:0.169036	eval-rmspe:0.145566 [74]	train-rmspe:0.168827	eval-rmspe:0.145480 [75]	train-rmspe:0.168700	eval-rmspe:0.145434 [76]	train-rmspe:0.168495	eval-rmspe:0.145501 [77]	train-rmspe:0.168024	eval-rmspe:0.145299 [78]	train-rmspe:0.167787	eval-rmspe:0.145228 [79]	train-rmspe:0.167442	eval-rmspe:0.145138 [80]	train-rmspe:0.167294	eval-rmspe:0.145091 [81]	train-rmspe:0.167094	eval-rmspe:0.145111 [82]	train-rmspe:0.166284	eval-rmspe:0.145054 [83]	train-rmspe:0.165992	eval-rmspe:0.145033 [84]	train-rmspe:0.165834	eval-rmspe:0.145011 [85]	train-rmspe:0.165299	eval-rmspe:0.144949 [86]	train-rmspe:0.165101	eval-rmspe:0.144845 [87]	train-rmspe:0.165050	eval-rmspe:0.144798 [88]	train-rmspe:0.164846	eval-rmspe:0.144714 [89]	train-rmspe:0.164813	eval-rmspe:0.144672 [90]	train-rmspe:0.164577	eval-rmspe:0.144524 [91]	train-rmspe:0.164520	eval-rmspe:0.144258 [92]	train-rmspe:0.164875	eval-rmspe:0.144230 [93]	train-rmspe:0.164647	eval-rmspe:0.144210 [94]	train-rmspe:0.164778	eval-rmspe:0.144138 [95]	train-rmspe:0.164575	eval-rmspe:0.143939 [96]	train-rmspe:0.164458	eval-rmspe:0.143812 [97]	train-rmspe:0.164444	eval-rmspe:0.143790 [98]	train-rmspe:0.164387	eval-rmspe:0.143728 [99]	train-rmspe:0.164229	eval-rmspe:0.143613 [100]	train-rmspe:0.164140	eval-rmspe:0.143462 [101]	train-rmspe:0.163355	eval-rmspe:0.143425 [102]	train-rmspe:0.163178	eval-rmspe:0.143481 [103]	train-rmspe:0.163078	eval-rmspe:0.143614 [104]	train-rmspe:0.162897	eval-rmspe:0.143514 [105]	train-rmspe:0.162607	eval-rmspe:0.143375 [106]	train-rmspe:0.160648	eval-rmspe:0.143548 [107]	train-rmspe:0.160512	eval-rmspe:0.143454 [108]	train-rmspe:0.160521	eval-rmspe:0.143425 [109]	train-rmspe:0.160269	eval-rmspe:0.143457 [110]	train-rmspe:0.160111	eval-rmspe:0.143483 [111]	train-rmspe:0.159996	eval-rmspe:0.143514 [112]	train-rmspe:0.159819	eval-rmspe:0.143577 [113]	train-rmspe:0.159420	eval-rmspe:0.143550 [114]	train-rmspe:0.159381	eval-rmspe:0.143510 [115]	train-rmspe:0.159342	eval-rmspe:0.143478 [116]	train-rmspe:0.159671	eval-rmspe:0.143517 [117]	train-rmspe:0.159342	eval-rmspe:0.143520 [118]	train-rmspe:0.159645	eval-rmspe:0.143547 [119]	train-rmspe:0.159526	eval-rmspe:0.143431 [120]	train-rmspe:0.159478	eval-rmspe:0.143438 [121]	train-rmspe:0.159318	eval-rmspe:0.143397 [122]	train-rmspe:0.159299	eval-rmspe:0.143390 [123]	train-rmspe:0.158301	eval-rmspe:0.143479 [124]	train-rmspe:0.158183	eval-rmspe:0.143487 [125]	train-rmspe:0.158079	eval-rmspe:0.143448 [126]	train-rmspe:0.157830	eval-rmspe:0.143397 [127]	train-rmspe:0.157158	eval-rmspe:0.143390 [128]	train-rmspe:0.157094	eval-rmspe:0.143377 [129]	train-rmspe:0.156987	eval-rmspe:0.143388 [130]	train-rmspe:0.151429	eval-rmspe:0.143381 [131]	train-rmspe:0.151408	eval-rmspe:0.143364 [132]	train-rmspe:0.148973	eval-rmspe:0.143401 [133]	train-rmspe:0.148952	eval-rmspe:0.143384 [134]	train-rmspe:0.148829	eval-rmspe:0.143352 [135]	train-rmspe:0.148641	eval-rmspe:0.143237 [136]	train-rmspe:0.148565	eval-rmspe:0.143186 [137]	train-rmspe:0.148507	eval-rmspe:0.143188 [138]	train-rmspe:0.148413	eval-rmspe:0.143307 [139]	train-rmspe:0.148351	eval-rmspe:0.143272 [140]	train-rmspe:0.137092	eval-rmspe:0.143435 [141]	train-rmspe:0.136920	eval-rmspe:0.143311 [142]	train-rmspe:0.136704	eval-rmspe:0.143300 [143]	train-rmspe:0.136225	eval-rmspe:0.143215 [144]	train-rmspe:0.136258	eval-rmspe:0.143196 [145]	train-rmspe:0.136119	eval-rmspe:0.143125 [146]	train-rmspe:0.136000	eval-rmspe:0.143112 [147]	train-rmspe:0.133141	eval-rmspe:0.143208 [148]	train-rmspe:0.133170	eval-rmspe:0.143118 [149]	train-rmspe:0.133090	eval-rmspe:0.143169 [150]	train-rmspe:0.132996	eval-rmspe:0.143203 [151]	train-rmspe:0.133006	eval-rmspe:0.143150 [152]	train-rmspe:0.132995	eval-rmspe:0.143187 [153]	train-rmspe:0.132908	eval-rmspe:0.143234 [154]	train-rmspe:0.132835	eval-rmspe:0.143239 [155]	train-rmspe:0.132708	eval-rmspe:0.143271 [156]	train-rmspe:0.132626	eval-rmspe:0.143327 [157]	train-rmspe:0.132622	eval-rmspe:0.143341 [158]	train-rmspe:0.132451	eval-rmspe:0.143311 [159]	train-rmspe:0.132287	eval-rmspe:0.143375 [160]	train-rmspe:0.132208	eval-rmspe:0.143329 [161]	train-rmspe:0.132163	eval-rmspe:0.143301 [162]	train-rmspe:0.132063	eval-rmspe:0.143341 [163]	train-rmspe:0.131688	eval-rmspe:0.143378 [164]	train-rmspe:0.131449	eval-rmspe:0.143364 [165]	train-rmspe:0.131358	eval-rmspe:0.143342 [166]	train-rmspe:0.131365	eval-rmspe:0.143343 [167]	train-rmspe:0.131262	eval-rmspe:0.143385 [168]	train-rmspe:0.131314	eval-rmspe:0.143358 [169]	train-rmspe:0.131318	eval-rmspe:0.143348 [170]	train-rmspe:0.131276	eval-rmspe:0.143324 [171]	train-rmspe:0.131008	eval-rmspe:0.143326 [172]	train-rmspe:0.130993	eval-rmspe:0.143337 [173]	train-rmspe:0.130953	eval-rmspe:0.143288 [174]	train-rmspe:0.130646	eval-rmspe:0.143313 [175]	train-rmspe:0.130648	eval-rmspe:0.143296 [176]	train-rmspe:0.130592	eval-rmspe:0.143311 [177]	train-rmspe:0.130516	eval-rmspe:0.143377 [178]	train-rmspe:0.130448	eval-rmspe:0.143397 [179]	train-rmspe:0.130380	eval-rmspe:0.143446 [180]	train-rmspe:0.130387	eval-rmspe:0.143421 [181]	train-rmspe:0.130331	eval-rmspe:0.143396 [182]	train-rmspe:0.130466	eval-rmspe:0.143400 [183]	train-rmspe:0.130448	eval-rmspe:0.143417 [184]	train-rmspe:0.130395	eval-rmspe:0.143414 [185]	train-rmspe:0.130400	eval-rmspe:0.143460 [186]	train-rmspe:0.130392	eval-rmspe:0.143456 [187]	train-rmspe:0.130381	eval-rmspe:0.143454 [188]	train-rmspe:0.130313	eval-rmspe:0.143567 [189]	train-rmspe:0.130153	eval-rmspe:0.143666 [190]	train-rmspe:0.130021	eval-rmspe:0.143665 [191]	train-rmspe:0.129912	eval-rmspe:0.143674 [192]	train-rmspe:0.129891	eval-rmspe:0.143676 [193]	train-rmspe:0.129855	eval-rmspe:0.143633 [194]	train-rmspe:0.129747	eval-rmspe:0.143691 [195]	train-rmspe:0.129685	eval-rmspe:0.143713 [196]	train-rmspe:0.129631	eval-rmspe:0.143677 Stopping. Best iteration: [146]	train-rmspe:0.136000	eval-rmspe:0.143112  In [ ]: ?In [ ]: ?In [ ]: ?Fit a Random ForestIn [46]: from sklearn.ensemble import RandomForestRegressor?rf = RandomForestRegressor().fit(X_train, y_train)In [47]: predictions = rf.predict(X_test)In [48]: %matplotlib inlinefeature_importance = rf.feature_importances_# make importances relative to max importancefeature_importance = 100.0 * (feature_importance / feature_importance.max())sorted_idx = np.argsort(feature_importance)pos = np.arange(sorted_idx.shape[0]) + .5plt.subplot(1, 2, 2)plt.barh(pos, feature_importance[sorted_idx], align='center')plt.yticks(pos, X_train.columns[sorted_idx])plt.xlabel('Relative Importance')plt.title('Variable Importance')plt.show()In [49]: from sklearn.metrics import mean_squared_error?mean_squared_error(predictions, y_test)Out[49]:0.023487981624745993Cross Validation TimeIn [50]: from sklearn.cross_validation import cross_val_score, cross_val_predict?cv_pred = cross_val_predict(rf, X_train, y_train, cv=5, n_jobs=-1)In [51]: mean_squared_error(cv_pred, y_train)Out[51]:0.023673221161816613In [52]: #cv_scores = cross_val_score(rf, X_train, y_train, cv=5)In [18]: cv_scoresOut[18]:array([ 0.82619497,  0.82981324,  0.83280576,  0.83177829,  0.82525807])Got Another Shit Result, Trying AdaBoostIn [ ]: ?In [ ]: ?Bring in the Evaluation SetIn [56]: test = pd.read_csv('../cleaneddata/cleanedtest.csv', index_col=0)test.head()# test.describe()Out[56]:0123456789...1314DayOfWeekStoreWeekNumberOpenIdObservedTrendMeanCustomerObserved00110001000...004138118.4464898.4467128.44648910110001000...00313818578.4464898.4467128.44648920110001000...002138117138.4464898.4467128.44648930110001000...001138125698.4464898.4467128.44648941010001000...007137034258.4464898.4467128.4464895 rows ? 23 columnsval_size = 15000params = {"objective": "reg:linear", "eta": 0.2, "max_depth": 12, "subsample": 0.7, "colsample_bytree": 0.7, "silent": 1 } num_trees = 2000 dtrain = xgb.DMatrix(X_train, y_train) dvalid = xgb.DMatrix(X_test, y_test) dtest = xgb.DMatrix(test) watchlist = [(dtrain, 'train'),(dvalid, 'eval')] gbm5 = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)In [57]: #dtest = xgb.DMatrix(test)In [58]: dtest = xgb.DMatrix(test.drop(['Open','Id'], axis=1))In [59]: pred_log_sales = gbm5.predict(dtest)In [61]: test['Sales'] = np.exp(pred_log_sales)In [62]: test.ix[test['Open'] == 0, 'Sales'] = 0In [63]: test.ix[:, ['Id', 'Sales']].to_csv('xgboostv6.csv', index=False, header=['Id', 'Sales'])In [66]: test.describe()Out[66]:0123456789...14DayOfWeekStoreWeekNumberOpenIdObservedTrendMeanCustomerObservedSalescount41088.00000041088.00000041088.00000041088.000000410884108841088.00000041088.00000041088.00000041088.000000...41088.00000041088.00000041088.00000041088.00000041088.00000041088.00000041088.00000041088.00000041088.00000041088.000000mean0.6041670.3958330.9956190.004381000.5565130.4434870.5385510.014019...0.4953273.979167555.89953334.6458330.85436120544.5000008.7481588.7482008.7481585635.969238std0.4890350.4890350.0660440.066044000.4968020.4968020.4985180.117569...0.4999842.015481320.2744962.0154810.35274811861.2282670.3050930.3039130.3050933379.678683min0.0000000.0000000.0000000.000000000.0000000.0000000.0000000.000000...0.0000001.0000001.00000031.0000000.0000001.0000007.8411707.8458577.8411700.00000025%0.0000000.0000001.0000000.000000000.0000000.0000000.0000000.000000...0.0000002.000000279.75000033.0000001.00000010272.7500008.5378358.5384398.5378354013.34924350%1.0000000.0000001.0000000.000000001.0000000.0000001.0000000.000000...0.0000004.000000553.50000035.0000001.00000020544.5000008.7433638.7425188.7433635662.74487375%1.0000001.0000001.0000000.000000001.0000001.0000001.0000000.000000...1.0000006.000000832.25000036.0000001.00000030816.2500008.9305268.9296238.9305267455.414673max1.0000001.0000001.0000001.000000001.0000001.0000001.0000001.000000...1.0000007.0000001115.00000038.0000001.00000041088.0000009.9224779.9180749.92247732679.5117198 rows ? 24 columnsIn [ ]: ?ImportsIn [39]:import pandas as pd import numpy as np import matplotlib.pyplot as plt Data ImportIn [40]:train = pd.read_csv('../data/train.csv') In [ ]:  Trim off the zero salesIn [41]:non_zero_sales_mask = train['Sales'] > 0  nz_train = train.ix[non_zero_sales_mask, :] nz_train.ix[:, 'LogSales'] = np.log(nz_train['Sales']) In [42]:nz_train.head() Out[42]:StoreDayOfWeekDateSalesCustomersOpenPromoStateHolidaySchoolHolidayLogSales0152015-07-31526355511018.5684561252015-07-31606462511018.7101252352015-07-31831482111019.0256963452015-07-3113995149811019.5464554552015-07-31482255911018.480944In [43]:for x in ['Open', 'Promo', 'StateHoliday', 'SchoolHoliday']:     print x, "=", np.unique(nz_train[x]) Open = [1] Promo = [0 1] StateHoliday = [0 '0' 'a' 'b' 'c'] SchoolHoliday = [0 1] In [44]:print(len(train), len(nz_train)) (1017209, 844338) Log Transforming SalesIn [45]:%pylab inline _, _, _ = plt.hist(nz_train.ix[non_zero_sales_mask, 'Sales'])   plt.figure() _, _, _ = plt.hist(nz_train.ix[non_zero_sales_mask, 'LogSales']) Populating the interactive namespace from numpy and matplotlib Add Weeknumber (most predictive date field)In [46]:import datetime as dt  nz_train['WeekNumber'] = nz_train['Date'].apply(lambda x: pd.to_datetime(x).isocalendar()[1]) /Users/Matt/anaconda2/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy   app.launch_new_instance() Encode State HolidayIn [47]:from sklearn.preprocessing import LabelEncoder nz_train.ix[:, ['StateHoliday']] = nz_train.ix[:, ['StateHoliday']].astype(str)  le = LabelEncoder().fit(np.unique(nz_train['StateHoliday']))  nz_train.ix[:, 'StateHolidayTransform'] = le.transform(nz_train.ix[:, 'StateHoliday']) In [48]:nz_train.head() Out[48]:StoreDayOfWeekDateSalesCustomersOpenPromoStateHolidaySchoolHolidayLogSalesWeekNumberStateHolidayTransform0152015-07-31526355511018.5684563101252015-07-31606462511018.7101253102352015-07-31831482111019.0256963103452015-07-3113995149811019.5464553104552015-07-31482255911018.480944310In [49]:np.unique(nz_train['StateHolidayTransform']) Out[49]:array([0, 1, 2, 3])Encode the Store DataIn [50]:store = pd.read_csv('../data/store.csv') In [51]:store.head() Out[51]:StoreStoreTypeAssortmentCompetitionDistanceCompetitionOpenSinceMonthCompetitionOpenSinceYearPromo2Promo2SinceWeekPromo2SinceYearPromoInterval01ca1270920080NaNNaNNaN12aa5701120071132010Jan,Apr,Jul,Oct23aa141301220061142011Jan,Apr,Jul,Oct34cc620920090NaNNaNNaN45aa29910420150NaNNaNNaNIn [52]:store.ix[:, 'StoreTypeTransformed'] = LabelEncoder().fit_transform(store['StoreType']) In [53]:store.ix[:, 'AssortmentTransformed'] = LabelEncoder().fit_transform(store['Assortment']) Attempt to JoinIn [ ]:  In [54]:joined = nz_train.merge(store, how='inner') In [55]:joined.dtypes Out[55]:Store                          int64 DayOfWeek                      int64 Date                          object Sales                          int64 Customers                      int64 Open                           int64 Promo                          int64 StateHoliday                  object SchoolHoliday                  int64 LogSales                     float64 WeekNumber                     int64 StateHolidayTransform          int64 StoreType                     object Assortment                    object CompetitionDistance          float64 CompetitionOpenSinceMonth    float64 CompetitionOpenSinceYear     float64 Promo2                         int64 Promo2SinceWeek              float64 Promo2SinceYear              float64 PromoInterval                 object StoreTypeTransformed           int64 AssortmentTransformed          int64 dtype: objectOneHotEconderIn [56]:from sklearn.preprocessing import OneHotEncoder  enc = OneHotEncoder().fit(joined.ix[:, ['Promo',                                         'StateHolidayTransform',                                          'SchoolHoliday',                                         'StoreTypeTransformed',                                         'AssortmentTransformed']])  categoricals= enc.transform(joined.ix[:, ['Promo',                                           'StateHolidayTransform',                                            'SchoolHoliday',                                           'StoreTypeTransformed',                                           'AssortmentTransformed']]).toarray().astype(float64)  training_df = pd.DataFrame(categoricals) training_df['LogSales'] = joined['LogSales'] training_df['DayOfWeek'] = joined['DayOfWeek'] training_df['Store'] = joined['Store'] training_df['WeekNumber'] = joined['WeekNumber'] training_df['Customers'] = joined['Customers'] training_df['Date'] = joined['Date'] In [ ]:  In [ ]:  In [57]:training_df.describe() Out[57]:01234567891011121314LogSalesDayOfWeekStoreWeekNumberCustomerscount844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000mean0.5536440.4463560.9989220.0008220.0001720.0000840.8064220.1935780.5413020.0184290.1337950.3064740.5268920.0097220.4633868.7575643.520350558.42137423.646946762.777166std0.4971140.4971140.0328120.0286580.0131040.0091700.3951020.3951020.4982920.1344960.3404320.4610290.4992770.0981220.4986580.4252781.723712321.73086114.389931401.194153min0.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000003.8286411.0000001.0000001.0000008.00000025%0.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000008.4885882.000000280.00000011.000000519.00000050%1.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000000.0000008.7591983.000000558.00000023.000000676.00000075%1.0000001.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.0000000.0000001.0000001.0000000.0000001.0000009.0312145.000000837.00000035.000000893.000000max1.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.00000010.6346777.0000001115.00000052.0000007388.000000In [ ]:  In [58]:#traindata2 = pd.merge(training_df, result2, on=['MoDay', 'Store']) In [59]:training_df.head() Out[59]:0123456789...11121314LogSalesDayOfWeekStoreWeekNumberCustomersDate00110000100...01008.56845651315552015-07-3110110000100...01008.52118541315462015-07-3020110000100...01008.47261431315232015-07-2930110000100...01008.51939121315602015-07-2840110000100...01008.71637211316122015-07-275 rows ? 21 columnsIn [60]:#create a datetime field so i can split it up into Month-Day. MoDay and Store will be a composite key training_df['Date']= pd.to_datetime(training_df['Date'], format="%Y-%m-%d") #training_df['MoDay']= training_df['Date'].dt.strftime('%m-%d') In [61]:training_df.describe() Out[61]:01234567891011121314LogSalesDayOfWeekStoreWeekNumberCustomerscount844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000mean0.5536440.4463560.9989220.0008220.0001720.0000840.8064220.1935780.5413020.0184290.1337950.3064740.5268920.0097220.4633868.7575643.520350558.42137423.646946762.777166std0.4971140.4971140.0328120.0286580.0131040.0091700.3951020.3951020.4982920.1344960.3404320.4610290.4992770.0981220.4986580.4252781.723712321.73086114.389931401.194153min0.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000003.8286411.0000001.0000001.0000008.00000025%0.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000008.4885882.000000280.00000011.000000519.00000050%1.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000000.0000008.7591983.000000558.00000023.000000676.00000075%1.0000001.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.0000000.0000001.0000001.0000000.0000001.0000009.0312145.000000837.00000035.000000893.000000max1.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.00000010.6346777.0000001115.00000052.0000007388.000000In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [62]:decomp = training_df In [63]:decomp['Date']= pd.to_datetime(decomp['Date'], format="%Y-%m-%d") In [64]:decomp = decomp.set_index(pd.DatetimeIndex(decomp['Date'])) In [65]:#interpolate missing values #decomp['LogSales'].interpolate(inplace=True) In [75]:#decompose time series from statsmodels.tsa.seasonal import seasonal_decompose decomposition = seasonal_decompose(decomp['LogSales'], model='additive', freq=12)   decomposition2 = seasonal_decompose(decomp['Customers'], model='additive', freq=12) In [37]:#%pylab inline #fig = plt.figure()   #fig = decomposition.plot()   Populating the interactive namespace from numpy and matplotlib <matplotlib.figure.Figure at 0x10bb6d4d0>In [ ]:  In [76]:result = pd.concat([decomposition.observed, decomposition.trend], axis=1) result.columns = ['Observed','Trend'] resultCustomers = pd.concat([decomposition.observed], axis=1) resultCustomers.columns = ['MeanCustomerObserved'] In [78]:#fill missing values #result = result.fillna(0) result['Store'] = decomp['Store'] resultCustomers['Store'] = decomp['Store'] In [79]:result2 = result.groupby(['Store']).mean() result2['Store'] = result2.index resultCustomers2 = resultCustomers.groupby(['Store']).mean() resultCustomers2['Store'] = resultCustomers2.index #result2['DayOfWeek'] = result2.index.get_level_values(0) #result2['Store'] = result2.index.get_level_values(1)  #use Month-Day - it didnt work #result['MoDay'] = result.index.strftime("%m-%d") #result2 = result.groupby(['MoDay','Store']).mean() #result2 = result.groupby(['MoDay','Store']).mean() #result2['MoDay'] = result2.index.get_level_values(0) #result2['Store'] = result2.index.get_level_values(1) In [ ]:  In [ ]:  In [70]:#mean observed, seasonal, trend, resid for each store, per day #traindata2.head() In [90]:#create a new training df by merging the mean results with the originail df #traindata2 = pd.merge(training_df, result2, on=['MoDay', 'Store']) #traindata2 = pd.merge(training_df, result2, on=['Store']) traindata2 = pd.merge(training_df,result2,how='left',on=['Store']) traindata3 = pd.merge(traindata2,resultCustomers2,how='left',on=['Store']) In [91]:traindata3.describe() Out[91]:0123456789...1314LogSalesDayOfWeekStoreWeekNumberCustomersObservedTrendMeanCustomerObservedcount844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000...844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000844338.000000mean0.5536440.4463560.9989220.0008220.0001720.0000840.8064220.1935780.5413020.018429...0.0097220.4633868.7575643.520350558.42137423.646946762.7771668.7575648.7575658.757564std0.4971140.4971140.0328120.0286580.0131040.0091700.3951020.3951020.4982920.134496...0.0981220.4986580.4252781.723712321.73086114.389931401.1941530.3213500.3200970.321350min0.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.0000000.000000...0.0000000.0000003.8286411.0000001.0000001.0000008.0000007.8058207.8098417.80582025%0.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000000.0000000.000000...0.0000000.0000008.4885882.000000280.00000011.000000519.0000008.5404438.5402838.54044350%1.0000000.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.000000...0.0000000.0000008.7591983.000000558.00000023.000000676.0000008.7619518.7624758.76195175%1.0000001.0000001.0000000.0000000.0000000.0000001.0000000.0000001.0000000.000000...0.0000001.0000009.0312145.000000837.00000035.000000893.0000008.9488218.9468718.948821max1.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.0000001.000000...1.0000001.00000010.6346777.0000001115.00000052.0000007388.0000009.9623269.9573879.9623268 rows ? 23 columnsIn [92]:traindata3 = traindata3.drop(['Customers','Date'],axis=1) In [93]:traindata3.dtypes Out[93]:0                       float64 1                       float64 2                       float64 3                       float64 4                       float64 5                       float64 6                       float64 7                       float64 8                       float64 9                       float64 10                      float64 11                      float64 12                      float64 13                      float64 14                      float64 LogSales                float64 DayOfWeek                 int64 Store                     int64 WeekNumber                int64 Observed                float64 Trend                   float64 MeanCustomerObserved    float64 dtype: objectIn [193]:#merge result3 with training set In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [ ]:  In [194]:#important_weeks = [18,19,20,21,22,23,24,25,26,27,28,29,30,31] In [195]:#customers_means = training_df[training_df['WeekNumber' == important_weeks]].groupby('Store').mean().Customers #customers_means.name = 'CustomersMean' In [196]:#training_df = training_df.join(customers_means, on='Store') In [120]:# training_df = joined.ix[:, ['Promo', #                             'Promo2', #                             'StateHolidayTransform',  #                             'SchoolHoliday', #                             'StoreTypeTransformed', #                             'AssortmentTransformed', #                             'LogSales', #                             'DayOfWeek', #                             'Store', #                             'WeekNumber']]  traindata3.to_csv('../cleaneddata/cleanedTraining.csv') Make Evaluation SetIn [95]:test = pd.read_csv('../data/test.csv') In [96]:test.head() Out[96]:IdStoreDayOfWeekDateOpenPromoStateHolidaySchoolHoliday01142015-09-17110012342015-09-17110023742015-09-17110034842015-09-17110045942015-09-171100Fix the unknown opensIn [97]:test.ix[test['Open'] != 0, 'Open'] = 1 In [98]:test.describe() Out[98]:IdStoreDayOfWeekOpenPromoSchoolHolidaycount41088.00000041088.00000041088.00000041088.00000041088.00000041088.000000mean20544.500000555.8995333.9791670.8543610.3958330.443487std11861.228267320.2744962.0154810.3527480.4890350.496802min1.0000001.0000001.0000000.0000000.0000000.00000025%10272.750000279.7500002.0000001.0000000.0000000.00000050%20544.500000553.5000004.0000001.0000000.0000000.00000075%30816.250000832.2500006.0000001.0000001.0000001.000000max41088.0000001115.0000007.0000001.0000001.0000001.000000In [99]:test['WeekNumber'] = test['Date'].apply(lambda x: pd.to_datetime(x).isocalendar()[1]) test.ix[:, ['StateHoliday']] = test.ix[:, ['StateHoliday']].astype(str)  test.ix[:, 'StateHolidayTransform'] = LabelEncoder().fit_transform(test['StateHoliday']) In [100]:joined_test = test.merge(store, how='inner') joined_test.head() Out[100]:IdStoreDayOfWeekDateOpenPromoStateHolidaySchoolHolidayWeekNumberStateHolidayTransform...AssortmentCompetitionDistanceCompetitionOpenSinceMonthCompetitionOpenSinceYearPromo2Promo2SinceWeekPromo2SinceYearPromoIntervalStoreTypeTransformedAssortmentTransformed01142015-09-171100380...a1270920080NaNNaNNaN201857132015-09-161100380...a1270920080NaNNaNNaN2021713122015-09-151100380...a1270920080NaNNaNNaN2032569112015-09-141100380...a1270920080NaNNaNNaN2043425172015-09-130000370...a1270920080NaNNaNNaN205 rows ? 21 columnsIn [ ]:  In [101]:#joined_test['Date']= pd.to_datetime(joined_test['Date'], format="%Y-%m-%d") #joined_test['MoDay']= joined_test['Date'].dt.strftime('%m-%d') In [ ]:  In [ ]:In [102]:joined_test.head() Out[102]:IdStoreDayOfWeekDateOpenPromoStateHolidaySchoolHolidayWeekNumberStateHolidayTransform...AssortmentCompetitionDistanceCompetitionOpenSinceMonthCompetitionOpenSinceYearPromo2Promo2SinceWeekPromo2SinceYearPromoIntervalStoreTypeTransformedAssortmentTransformed01142015-09-171100380...a1270920080NaNNaNNaN201857132015-09-161100380...a1270920080NaNNaNNaN2021713122015-09-151100380...a1270920080NaNNaNNaN2032569112015-09-141100380...a1270920080NaNNaNNaN2043425172015-09-130000370...a1270920080NaNNaNNaN205 rows ? 21 columnsIn [103]:categoricals = enc.transform(joined_test.ix[:, ['Promo',                                                 'StateHolidayTransform',                                                  'SchoolHoliday',                                                 'StoreTypeTransformed',                                                 'AssortmentTransformed']]).toarray().astype(float64) testing_df = pd.DataFrame(categoricals) testing_df['DayOfWeek'] = joined_test['DayOfWeek'] testing_df['Store'] = joined_test['Store'] testing_df['WeekNumber'] = joined_test['WeekNumber'] testing_df['Open'] = joined_test['Open'] testing_df['Id'] = joined_test['Id'] #testing_df['Date'] = joined_test['Date'] In [104]:#date time manipulation - MoDay and Store will be used as a composite key #testing_df['Date']= pd.to_datetime(testing_df['Date'], format="%Y-%m-%d") #testing_df['MoDay']= testing_df['Date'].dt.strftime('%m-%d') #testing_df['Store2'] = testing_df['Store'] In [105]:testing_df.dtypes Out[105]:0             float64 1             float64 2             float64 3             float64 4             float64 5             float64 6             float64 7             float64 8             float64 9             float64 10            float64 11            float64 12            float64 13            float64 14            float64 DayOfWeek       int64 Store           int64 WeekNumber      int64 Open          float64 Id              int64 dtype: objectIn [111]:result2.tail() Out[111]:ObservedTrendStoreStore11118.5176808.518393111111129.1871669.185016111211138.7778878.781502111311149.9224779.918074111411158.7116548.7141181115In [108]:#result2 is the mean of the decomposed time series, per month-day #missing values - have to use outer join  #testing_df2 = pd.merge(testing_df,result2,how='left',on=['MoDay','Store']) testing_df2 = pd.merge(testing_df,result2,how='left',on=['Store']) In [115]:testing_df3 = pd.merge(testing_df2,resultCustomers2,how='left',on=['Store']) In [116]:testing_df3.head() Out[116]:0123456789...1314DayOfWeekStoreWeekNumberOpenIdObservedTrendMeanCustomerObserved00110001000...004138118.4464898.4467128.44648910110001000...00313818578.4464898.4467128.44648920110001000...002138117138.4464898.4467128.44648930110001000...001138125698.4464898.4467128.44648941010001000...007137034258.4464898.4467128.4464895 rows ? 23 columnsIn [154]:#testing_df3 = testing_df2[np.isfinite(testing_df2['Store2'])] In [112]:#double check the join because i screwed it up In [164]:#testing_df3 = testing_df3.fillna(testing_df3.mean()) In [165]:#result2[(result2['Store']==13) & (result2['MoDay']=="09-08")] In [117]:testing_df3[testing_df3['Observed'].isnull()==True] Out[117]:0123456789...1314DayOfWeekStoreWeekNumberOpenIdObservedTrendMeanCustomerObserved0 rows ? 23 columnsIn [121]:testing_df3[2000:2001] Out[121]:0123456789...1314DayOfWeekStoreWeekNumberOpenIdObservedTrendMeanCustomerObserved20001010001010...01753330274348.531328.5328258.531321 rows ? 23 columnsIn [ ]:  In [119]:testing_df3.to_csv('../cleaneddata/cleanedtest.csv') In [ ]: 